{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ab1ed1b-bd18-4d4e-88af-c70a335ebbef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import Anthropic, HuggingFaceHub\n",
    "from langchain import PromptTemplate, LLMChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811a3bc3-34ef-4e89-8cca-d0048efc9af9",
   "metadata": {},
   "source": [
    "# Simple test of llm library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "405ec25f-e640-4e12-ac19-4279193e1160",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = \"What would be a good company name for a company that makes colorful socks?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d88dab40-d911-499f-8c0c-5514fa877993",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"Who won the Superbowl the year Paul McCartney sang during the half-time show?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7190b9de-ce46-485b-aa77-b4d3451f29ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d9a229-a243-40d1-ae95-c64d4fcb89c1",
   "metadata": {},
   "source": [
    "## Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2d4a1c62-db9c-4173-b75c-ece40e349217",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_anthropic = Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e51a2e64-3489-4b1d-88e2-b9d9065b0b9f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Snazzy Sock Company\n",
      "\n",
      "Colorful Crews (referring to socks that cover the crew length of the foot)\n",
      "\n",
      "Ankle Arcs (referring to the arched color patterns around the ankle)\n",
      "\n",
      "Toe Tints \n",
      "\n",
      "Sherbet Socks\n",
      "\n",
      "Prismatic Peds (ped refers to footwear)\n",
      "\n",
      "Kaleidoscope Krews \n",
      "\n",
      "Sock Spectrum\n",
      "\n",
      "Those are a few ideas playing around with colorful, bright and whimsical names that convey fun, fashionable socks. Let me know if you would like any additional options.\n"
     ]
    }
   ],
   "source": [
    "print(llm_anthropic(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0c704670-04f4-4267-9a91-f7717bab5478",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_anthropic_chain = LLMChain(prompt=prompt, llm=llm_anthropic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b7be54f4-c0d1-44c9-a8fd-b58d8becd50f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1.) Focus on the main attribute. Socks are bright and colorful. Some name options could be:\n",
      "\n",
      "- BrightSocks \n",
      "- ColorfulSocks\n",
      "- VibrantSocks\n",
      "\n",
      "Step 2.) Use creative and catchy words. Some options:\n",
      "\n",
      "- SockItToMe  (playing on the phrase 'sock it to me')\n",
      "- SockPop  \n",
      "- DreamSocks\n",
      "\n",
      "Step 3.) Include imagery or story. For example:\n",
      "\n",
      "- RainbowSocks (imagery of a rainbow)\n",
      "- ThreadTheNeedle (story of stitching/sewing colorful socks)\n",
      "\n",
      "Step 4.) Use word play or rhyme:\n",
      "\n",
      "- SockHop  (rhymes with sock stop) \n",
      "- SockItRealGood  (play on 'sock it to me')\n",
      "\n",
      "Step 5.) Include location or founder name:\n",
      "\n",
      "- ColoradoSocks (based in Colorado)\n",
      "- BettysBrightSocks (founders name is Betty)\n",
      "\n",
      "Those are a few options and steps I would suggest for brainstorming a creative company name for colorful socks. Have fun and good luck! Let me know if you have any other questions.\n"
     ]
    }
   ],
   "source": [
    "print(llm_anthropic_chain.run(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf473f35-2628-44c4-990c-891ed7753dfa",
   "metadata": {},
   "source": [
    "## HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "45485d79-4433-432a-850a-b7488e1ed391",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#repo_id = \"bigscience/bloomz\"\n",
    "#repo_id = \"databricks/dolly-v2-3b\"\n",
    "repo_id = \"google/flan-t5-xl\"\n",
    "#repo_id = \"OpenAssistant/stablelm-7b-sft-v7-epoch-3\"\n",
    "llm_hf = HuggingFaceHub(repo_id=repo_id, model_kwargs={\"temperature\":0, \"max_length\":32})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c35574cf-ac92-4e04-b9f8-f2234adf3b0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error raised by inference API: Model google/flan-t5-xl time out",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mllm_hf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/langchain/llms/base.py:246\u001b[0m, in \u001b[0;36mBaseLLM.__call__\u001b[0;34m(self, prompt, stop)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, prompt: \u001b[38;5;28mstr\u001b[39m, stop: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    245\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check Cache and run the LLM on the given prompt and input.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/langchain/llms/base.py:140\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mon_llm_end(output, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/langchain/llms/base.py:137\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m    134\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m}, prompts, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose\n\u001b[1;32m    135\u001b[0m )\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/langchain/llms/base.py:324\u001b[0m, in \u001b[0;36mLLM._generate\u001b[0;34m(self, prompts, stop)\u001b[0m\n\u001b[1;32m    322\u001b[0m generations \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[0;32m--> 324\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/langchain/llms/huggingface_hub.py:105\u001b[0m, in \u001b[0;36mHuggingFaceHub._call\u001b[0;34m(self, prompt, stop)\u001b[0m\n\u001b[1;32m    103\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient(inputs\u001b[38;5;241m=\u001b[39mprompt, params\u001b[38;5;241m=\u001b[39m_model_kwargs)\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m response:\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError raised by inference API: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mtask \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;66;03m# Text generation return includes the starter text.\u001b[39;00m\n\u001b[1;32m    108\u001b[0m     text \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;28mlen\u001b[39m(prompt) :]\n",
      "\u001b[0;31mValueError\u001b[0m: Error raised by inference API: Model google/flan-t5-xl time out"
     ]
    }
   ],
   "source": [
    "print(llm_hf(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3e4d61-69da-4b60-98bf-a271789ca51c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_hf_chain = LLMChain(prompt=prompt, llm=llm_hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdb7762-1713-4ad2-97a9-df689f513fcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_hf_chain.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f38b8df-deb7-4e54-8553-d85eb9f2e714",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
