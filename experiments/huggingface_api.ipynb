{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3b8dac6-60d8-4a58-8ef0-9f45f6f9abd7",
   "metadata": {},
   "source": [
    "# Via HuggingFace's API\n",
    "\n",
    "model zoo: https://huggingface.co/docs/api-inference/detailed_parameters\n",
    "\n",
    "quicktour: https://huggingface.co/docs/api-inference/quicktour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "653b6e13-26df-47da-8bb4-e8f533244577",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4975d94-337a-461a-93b9-02d73c22b071",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "HUGGINGFACEHUB_API_TOKEN=os.environ.get(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "headers = {\"Authorization\": f\"Bearer {HUGGINGFACEHUB_API_TOKEN}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "979e2ebd-2fd9-4f7d-8f48-1062dc57fd1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def query(payload, model_name):\n",
    "    data = json.dumps(payload)\n",
    "    API_URL = f\"https://api-inference.huggingface.co/models/{model_name}\"\n",
    "    response = requests.request(\"POST\", API_URL, headers=headers, data=data)\n",
    "    return json.loads(response.content.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b04068c-be4c-4957-8a0c-703e76a93419",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'error': 'overloaded'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q&A model\n",
    "model_name = \"deepset/roberta-base-squad2\"\n",
    "query(\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What's my name?\", \n",
    "                   \"context\": \"My name is Clara and I live in Berkeley.\", }\n",
    "    }, \n",
    "    model_name=model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43c101f4-3266-434e-b39a-4c5240d8cdd4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world.'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summarization\n",
    "model_name = \"facebook/bart-large-cnn\"\n",
    "query(\n",
    "    {\n",
    "        \"inputs\": \"The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, \"\n",
    "        \"and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. \"\n",
    "        \"During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, \"\n",
    "        \"a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. \"\n",
    "        \"It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, \"\n",
    "        \"it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, \"\n",
    "        \"the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.\",\n",
    "        \"parameters\": {\"do_sample\": False},\n",
    "    },\n",
    "    model_name=model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a42b42d5-78bd-4d5b-ab88-355ac4c7986d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6945773363113403, 0.9429150819778442, 0.2568760812282562]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentence similarity\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "query(\n",
    "    {\n",
    "        \"inputs\": {\n",
    "            \"source_sentence\": \"That is a happy person\",\n",
    "            \"sentences\": [\"That is a happy dog\", \"That is a very happy person\", \"Today is a sunny day\"],\n",
    "        }\n",
    "    }, \n",
    "    model_name=model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc98ee68-33a2-4759-9103-681e4d1e6226",
   "metadata": {},
   "source": [
    "# Via LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b06f30c8-8733-4313-ae8d-783d6e74058e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = \"What would be a good company name for a company that makes colorful socks?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71da4f28-e945-4fbc-9288-a8d3c1832c34",
   "metadata": {},
   "source": [
    "## HuggingFaceHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "57ad2914-9fc7-4bc0-9a01-b2b8a27d1d71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFaceHub\n",
    "from langchain import PromptTemplate, LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "71e81195-57c7-49b2-96b7-e3b6303e7cda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_hf = HuggingFaceHub(repo_id=model_name, model_kwargs={\"temperature\":0.01, \"max_length\":500})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f7902df3-dd3f-4cc5-8c5b-37c89d090b86",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error raised by inference API: Internal Server Error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mllm_hf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/langchain/llms/base.py:246\u001b[0m, in \u001b[0;36mBaseLLM.__call__\u001b[0;34m(self, prompt, stop)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, prompt: \u001b[38;5;28mstr\u001b[39m, stop: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    245\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check Cache and run the LLM on the given prompt and input.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/langchain/llms/base.py:140\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mon_llm_end(output, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/langchain/llms/base.py:137\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m    134\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m}, prompts, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose\n\u001b[1;32m    135\u001b[0m )\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/langchain/llms/base.py:324\u001b[0m, in \u001b[0;36mLLM._generate\u001b[0;34m(self, prompts, stop)\u001b[0m\n\u001b[1;32m    322\u001b[0m generations \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[0;32m--> 324\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/langchain/llms/huggingface_hub.py:105\u001b[0m, in \u001b[0;36mHuggingFaceHub._call\u001b[0;34m(self, prompt, stop)\u001b[0m\n\u001b[1;32m    103\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient(inputs\u001b[38;5;241m=\u001b[39mprompt, params\u001b[38;5;241m=\u001b[39m_model_kwargs)\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m response:\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError raised by inference API: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mtask \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;66;03m# Text generation return includes the starter text.\u001b[39;00m\n\u001b[1;32m    108\u001b[0m     text \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;28mlen\u001b[39m(prompt) :]\n",
      "\u001b[0;31mValueError\u001b[0m: Error raised by inference API: Internal Server Error"
     ]
    }
   ],
   "source": [
    "llm_hf(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fd4ec9-2619-4492-9562-c90603953697",
   "metadata": {},
   "source": [
    "## LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "315295b0-7be0-4f3d-967d-61ef96d9ff25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f137c299-73d8-49b8-8dfb-1fbcc3a65177",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"Who won the Superbowl the year Paul McCartney sang during the half-time show?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c3cd552e-4ff8-4f5f-8ff9-cbdf5ca08dbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_hf_chain = LLMChain(prompt=prompt, llm=llm_hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9b3b4f13-cb4a-41a0-983a-744128ef4c8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' First, the Superbowl is a football game. Second, the half-time show is a musical'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_hf_chain.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cb33e3-5c2c-49c0-9ce8-99b251f68580",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
